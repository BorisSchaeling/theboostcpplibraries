<?xml version="1.0" encoding="utf-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="5.0" xml:lang="en" xml:id="boost.mpi">
  <title>Boost.MPI</title><indexterm><primary>Boost.MPI</primary></indexterm>
  <para><link xlink:href="http://www.boost.org/libs/mpi">Boost.MPI</link> provides an interface to the <acronym>MPI</acronym> standard (Message Passing Interface). This standard simplifies the development of programs that execute tasks concurrently. You could develop such programs using threads or by making multiple processes communicate with each other through shared memory or network connections. The advantage of <acronym>MPI</acronym> is that you don’t need to take care of such details. You can fully concentrate on parallelizing your program.</para>
  <para>A disadvantage is that you need an <acronym>MPI</acronym> runtime environment. <acronym>MPI</acronym> is only an option if you control the runtime environment. For example, if you want to distribute a program that can be started with a double click, you won’t be able to use <acronym>MPI</acronym>. While operating systems support threads, shared memory, and networks out of the box, they usually don’t provide an <acronym>MPI</acronym> runtime environment. Users need to perform additional steps to start an <acronym>MPI</acronym> program.</para>
  <sect1 xml:id="boost.mpi-development-and-runtime-environment">
    <title>Development and Runtime Environment</title>
    <para><acronym>MPI</acronym> defines functions for <emphasis role="concept">parallel computing</emphasis><indexterm><primary>parallel computing, Boost.MPI</primary></indexterm>. Parallel computing refers to programs that can execute tasks concurrently in runtime environments that support the parallel execution of tasks. Such runtime environments are usually based on multiple processors. Because a single processor can only execute code sequentially, linking multiple processors creates a runtime environment that can execute tasks in parallel. If thousands of processors are linked, the result is a parallel computer &#x2013; a type of architecture usually found only in supercomputers. <acronym>MPI</acronym> comes from the search to find methods for programming supercomputers more easily.</para>
    <para>If you want to use <acronym>MPI</acronym>, you need an implementation of the standard. While <acronym>MPI</acronym> defines many functions, they are usually not supported by operating systems out of the box. For example, the desktop editions of Windows aren’t shipped with <acronym>MPI</acronym> support.</para>
    <para>The most important <acronym>MPI</acronym> implementations are <link xlink:href="http://www.mpich.org/">MPICH</link><indexterm><primary>MPICH, Boost.MPI</primary></indexterm> and <link xlink:href="http://www.open-mpi.org/">Open MPI</link><indexterm><primary>Open MPI, Boost.MPI</primary></indexterm>. MPICH is one of the earliest <acronym>MPI</acronym> implementations. It has existed since the mid 1990s. MPICH is a mature and portable implementation that is actively maintained and updated. The first version of Open MPI was released 2005. Because Open MPI is a collaborative effort that includes many developers who were responsible for earlier <acronym>MPI</acronym> implementations, Open MPI is seen as the future standard. However, that doesn’t mean that MPICH can be ignored. There are several <acronym>MPI</acronym> implementations that are based on MPICH. For example, Microsoft ships an <acronym>MPI</acronym> implementation called Microsoft HPC Pack that is based on MPICH.</para>
    <para>MPICH provides installation files for various operating systems, such as Windows, Linux, and OS X. If you need an <acronym>MPI</acronym> implementation and don’t want to build it from source code, the MPICH installation files are the fastest path to start using <acronym>MPI</acronym>.</para>
    <para>The MPICH installation files contain the required header files and libraries to develop <acronym>MPI</acronym> programs. Furthermore, they contain an <acronym>MPI</acronym> runtime environment. Because <acronym>MPI</acronym> programs execute tasks on several processors concurrently, they run in several processes. An <acronym>MPI</acronym> program is started multiple times, not just once. Several instances of the same <acronym>MPI</acronym> program run on multiple processors and communicate through functions defined by the <acronym>MPI</acronym> standard.</para>
    <para>You can’t start an <acronym>MPI</acronym> program with a double click. You use a helper program, which is usually called <application class="software">mpiexec</application><indexterm><primary>mpiexec, Boost.MPI</primary></indexterm>. You pass your <acronym>MPI</acronym> program to <application class="software">mpiexec</application>, which starts your program in the <acronym>MPI</acronym> runtime environment. Command line options determine how many processes are started and how they communicate &#x2013; for example, through sockets or shared memory. Because the <acronym>MPI</acronym> runtime environment takes care of these details, you can concentrate on parallel programming.</para>
    <para>If you decided to use the installation files from MPICH, note that MPICH only provides a 64-bit version. You must use a 64-bit compiler to develop <acronym>MPI</acronym> programs with MPICH and build a 64-bit version of Boost.MPI.</para>
  </sect1>
  <sect1 xml:id="boost.mpi-simple-data-exchange">
    <title>Simple Data Exchange</title>
    <para>Boost.MPI is a C++ interface to the <acronym>MPI</acronym> standard. The library uses the namespace <package>boost::mpi</package>. It is sufficient to include the header file <filename class="headerfile">boost/mpi.hpp</filename> to get access to all classes and functions.</para>
    <example xml:id="ex.mpi_01">
      <title><acronym>MPI</acronym> environment and communicator</title>
      <programlisting><xi:include href="../src/mpi_01/main.cpp" parse="text"/></programlisting>
    </example>
    <para><xref linkend="ex.mpi_01" xrefstyle="enp"/> is a simple <acronym>MPI</acronym> program. It uses two classes that you will find in all of the examples that follow. <classname>boost::mpi::environment</classname><indexterm><primary>mpi::environment</primary></indexterm><indexterm><primary>environment, mpi</primary></indexterm> initializes <acronym>MPI</acronym>. The constructor calls the function <function>MPI_Init</function><indexterm><primary>MPI_Init</primary></indexterm> from the <acronym>MPI</acronym> standard. The destructor calls <function>MPI_Finalize</function><indexterm><primary>MPI_Finalize</primary></indexterm>. <classname>boost::mpi::communicator</classname><indexterm><primary>mpi::communicator</primary></indexterm><indexterm><primary>communicator, mpi</primary></indexterm> is used to create a <emphasis role="concept">communicator</emphasis><indexterm><primary>communicator, Boost.MPI</primary></indexterm>. The communicator is one of the central concepts of <acronym>MPI</acronym> and supports data exchange between processes.</para>
    <para><classname>boost::mpi::environment</classname> is a very simple class that provides only a few member functions. You can call <methodname>initialized</methodname><indexterm><primary>mpi::environment::initialized</primary></indexterm><indexterm><primary>initialized, mpi::environment</primary></indexterm> to check whether <acronym>MPI</acronym> has been initialized successfully. The member function returns a value of type <type>bool</type>. <methodname>processor_name</methodname><indexterm><primary>mpi::environment::processor_name</primary></indexterm><indexterm><primary>processor_name, mpi::environment</primary></indexterm> returns the name of the current process as a <classname>std::string</classname>. And <methodname>abort</methodname><indexterm><primary>mpi::environment::abort</primary></indexterm><indexterm><primary>abort, mpi::environment</primary></indexterm> stops an <acronym>MPI</acronym> program, not just the current process. You pass an <type>int</type> value to <methodname>abort</methodname>. This value will be passed to the <acronym>MPI</acronym> runtime environment as the return value of the <acronym>MPI</acronym> program. For most <acronym>MPI</acronym> programs you won’t need these member functions. You usually instantiate <classname>boost::mpi::environment</classname> at the beginning of a program and then don’t use the object afterwards &#x2013; as in <xref linkend="ex.mpi_01" xrefstyle="enp"/> and the following examples in this chapter.</para>
    <para><classname>boost::mpi::communicator</classname> is more interesting. This class is a communicator that links the processes that are part of an <acronym>MPI</acronym> program. Every process has a rank, which is an integer &#x2013; all processes are enumerated. A process can discover its rank by calling <methodname>rank</methodname><indexterm><primary>mpi::communicator::rank</primary></indexterm><indexterm><primary>rank, mpi::communicator</primary></indexterm> on the communicator. If a process wants to know how many processes there are, it calls <methodname>size</methodname><indexterm><primary>mpi::communicator::size</primary></indexterm><indexterm><primary>size, mpi::communicator</primary></indexterm>.</para>
    <para>To run <xref linkend="ex.mpi_01" xrefstyle="enp"/>, you have to use a helper program provided by the <acronym>MPI</acronym> implementation you are using. With MPICH, the helper program is called <application class="software">mpiexec</application>. You can run <xref linkend="ex.mpi_01" xrefstyle="enp"/> using this helper with the following command:</para>
    <literallayout><command>mpiexec -n 4 sample.exe</command></literallayout>
    <para><application>mpiexec</application> expects the name of an <acronym>MPI</acronym> program and an option that tells it how many processes to launch. The option <literal>-n 4</literal> tells <application>mpiexec</application> to launch four processes. Thus the <acronym>MPI</acronym> program is started four times. However, the four processes aren’t independent. They are linked through the <acronym>MPI</acronym> runtime environment, and they all belong to the same communicator, which gave each process a rank. If you run <xref linkend="ex.mpi_01" xrefstyle="enp"/> with four processes, <methodname>rank</methodname> returns a number from 0 to 3 and <methodname>size</methodname> 4.</para>
    <para>Please note that the output can be mixed up. After all, four processes are writing to the standard output stream at the same time. For example, it’s unknown whether the process with rank 0, or any other rank, is the first one to write to the standard output stream. It’s also possible that one process will interrupt another one while writing to the standard output stream. The interrupted process might not be able to complete writing its rank and the size of the communicator before another process writes to the standard output stream, breaking up the output.</para>
    <example xml:id="ex.mpi_02">
      <title>Blocking functions to send and receive data</title>
      <programlisting><xi:include href="../src/mpi_02/main.cpp" parse="text"/></programlisting>
    </example>
    <para><classname>boost::mpi::communicator</classname> provides two simple member functions, <methodname>send</methodname><indexterm><primary>mpi::communicator::send</primary></indexterm><indexterm><primary>send, mpi::communicator</primary></indexterm> and <methodname>recv</methodname><indexterm><primary>mpi::communicator::recv</primary></indexterm><indexterm><primary>recv, mpi::communicator</primary></indexterm>, to exchange data between two processes. They are blocking functions that only return when data has been sent or received. This is especially important for <methodname>recv</methodname>. If <methodname>recv</methodname> is called without another process sending it data, the call blocks and the process will stall in the call.</para>
    <para>In <xref linkend="ex.mpi_02" xrefstyle="enp"/>, the process with rank 0 receives data with <methodname>recv</methodname>. The process with rank 1 sends data with <methodname>send</methodname>. If you start the program with more than two processes, the other processes exit without doing anything.</para>
    <para>You pass three parameters to <methodname>send</methodname>: The first parameter is the rank of the process to which data should be sent. The second parameter is a <emphasis role="concept">tag</emphasis><indexterm><primary>tag, Boost.MPI</primary></indexterm> to identify data. The third parameter is the data.</para>
    <para>The tag is always an integer. In <xref linkend="ex.mpi_02" xrefstyle="enp"/> the tag is 16. The tag makes it possible to identify a call to <methodname>send</methodname>. You’ll see that the tag is used with <methodname>recv</methodname>.</para>
    <para>The third parameter passed to <methodname>send</methodname> is 99. This number is sent from the process with rank 1 to the process with rank 0. Boost.MPI supports all primitive types. An <type>int</type> value like 99 can be sent directly.</para>
    <para><methodname>recv</methodname> expects similar parameters. The first parameter is the rank of the process from which data should be received. The second parameter is the tag that links the call to <methodname>recv</methodname> with the call to <methodname>send</methodname>. The third parameter is the variable to store the received data in.</para>
    <para>If you run <xref linkend="ex.mpi_02" xrefstyle="enp"/> with at least two processes, <computeroutput>99</computeroutput> is displayed.</para>
    <example xml:id="ex.mpi_03">
      <title>Receiving data from any process</title>
      <programlisting><xi:include href="../src/mpi_03/main.cpp" parse="text"/></programlisting>
    </example>
    <para><xref linkend="ex.mpi_03" xrefstyle="enp"/> is based on <xref linkend="ex.mpi_02" xrefstyle="enp"/>. Instead of sending the number 99, it sends the rank of the process that calls <methodname>send</methodname>. This could be any process with a rank greater than 0.</para>
    <para>The call to <methodname>recv</methodname> for the process with rank 0 has changed, too. <varname>boost::mpi::any_source</varname><indexterm><primary>mpi::any_source</primary></indexterm><indexterm><primary>any_source, mpi</primary></indexterm> is the first parameter. This means the call to <methodname>recv</methodname> will accept data from any process that sends data with the tag 16.</para>
    <para>If you start <xref linkend="ex.mpi_03" xrefstyle="enp"/> with two processes, <computeroutput>1</computeroutput> will be displayed. After all, there is only one process that can call <methodname>send</methodname> &#x2013; the process with rank 1. If you start the program with more than two processes, it’s unknown which number will be displayed. In this case, multiple processes will call <methodname>send</methodname> and try to send their rank. Which process is first and, therefore, which rank is displayed, is random.</para>
    <example xml:id="ex.mpi_04">
      <title>Detecting the sender with <classname>boost::mpi::status</classname></title>
      <programlisting><xi:include href="../src/mpi_04/main.cpp" parse="text"/></programlisting>
    </example>
    <para><methodname>recv</methodname> has a return value of type <classname>boost::mpi::status</classname><indexterm><primary>mpi::status</primary></indexterm><indexterm><primary>status, mpi</primary></indexterm>. This class provides a member function <methodname>source</methodname><indexterm><primary>mpi::status::source</primary></indexterm><indexterm><primary>source, mpi::status</primary></indexterm>, which returns the rank of the process from which data was received. <xref linkend="ex.mpi_04" xrefstyle="enp"/> tells you from which process the number 99 was received.</para>
    <para>So far, all examples have used <methodname>send</methodname> and <methodname>recv</methodname> to transmit an <type>int</type> value. In <xref linkend="ex.mpi_05" xrefstyle="enp"/> a string is transmitted.</para>
    <example xml:id="ex.mpi_05">
      <title>Transmitting an array with <methodname>send</methodname> and <methodname>recv</methodname></title>
      <programlisting><xi:include href="../src/mpi_05/main.cpp" parse="text"/></programlisting>
    </example>
    <para><methodname>send</methodname> and <methodname>recv</methodname> can transmit arrays as well as single values. <xref linkend="ex.mpi_05" xrefstyle="enp"/> transmits a string in a <type>char</type> array. Because <methodname>send</methodname> and <methodname>recv</methodname> support primitive types like <type>char</type>, the <type>char</type> array can be transmitted without any problems.</para>
    <para><methodname>send</methodname> takes a pointer to the string as its third parameter and the size of the string as its fourth parameter. The third parameter passed to <methodname>recv</methodname> is a pointer to an array to store the received data. The fourth parameter tells <methodname>recv</methodname> how many <type>char</type>s should be received and stored in <varname>buffer</varname>. <xref linkend="ex.mpi_05" xrefstyle="enp"/> writes <computeroutput>Hello, world!</computeroutput> to the standard output stream.</para>
    <example xml:id="ex.mpi_06">
      <title>Transmitting a string with <methodname>send</methodname> and <methodname>recv</methodname></title>
      <programlisting><xi:include href="../src/mpi_06/main.cpp" parse="text"/></programlisting>
    </example>
    <para>Even though Boost.MPI supports only primitive types, that doesn’t mean it’s impossible to transmit objects of non-primitive types. Boost.MPI works together with Boost.Serialization. Objects that can be serialized according to the rules of Boost.Serialization can be transmitted with Boost.MPI.</para>
    <para><xref linkend="ex.mpi_06" xrefstyle="enp"/> transmits <quote>Hello, world!</quote> This time the value transmitted is not a <type>char</type> array but a <classname>std::string</classname>. Boost.Serialization provides the header file <filename class="headerfile">boost/serialization/string.hpp</filename>, which only needs to be included to make <classname>std::string</classname> serializable.</para>
    <para>If you want to transmit objects of user-defined types, see <xref linkend="boost.serialization" xrefstyle="wtp"/>.</para>
  </sect1>
  <sect1 xml:id="boost.mpi-asynchronous-data-exchange">
    <title>Asynchronous data exchange</title>
    <para>In addition to the blocking functions <methodname>send</methodname> and <methodname>recv</methodname>, Boost.MPI also supports asynchronous data exchange with the member functions <methodname>isend</methodname><indexterm><primary>mpi::communicator::isend</primary></indexterm><indexterm><primary>isend, mpi::communicator</primary></indexterm> and <methodname>irecv</methodname><indexterm><primary>mpi::communicator::irecv</primary></indexterm><indexterm><primary>irecv, mpi::communicator</primary></indexterm>. The names start with an i to indicate that the functions return immediately.</para>
    <example xml:id="ex.mpi_07">
      <title>Receiving data asynchronously with <methodname>irecv</methodname></title>
      <programlisting><xi:include href="../src/mpi_07/main.cpp" parse="text"/></programlisting>
    </example>
    <para><xref linkend="ex.mpi_07" xrefstyle="enp"/> uses the blocking function <methodname>send</methodname> to send the string <quote>Hello, world!</quote> However, data is received with the asynchronous function <methodname>irecv</methodname>. This member function expects the same parameters as <methodname>recv</methodname>. The difference is that there is no guarantee that data has been received in <varname>s</varname> when <methodname>irecv</methodname> returns.</para>
    <para><methodname>irecv</methodname> returns an object of type <classname>boost::mpi::request</classname><indexterm><primary>mpi::request</primary></indexterm><indexterm><primary>request, mpi</primary></indexterm>. You can call <methodname>test</methodname><indexterm><primary>mpi::request::test</primary></indexterm><indexterm><primary>test, mpi::request</primary></indexterm> to check whether data has been received. This member function returns a <type>bool</type>. You can call <methodname>test</methodname> as often as you like. Because <methodname>irecv</methodname> is an asynchronous member function, it is possible that the first call will return <literal>false</literal> and the second <literal>true</literal>. This would mean that the asynchronous operation was completed between the two calls.</para>
    <para><xref linkend="ex.mpi_07" xrefstyle="enp"/> calls <methodname>test</methodname> only once. If data has been received in <varname>s</varname>, the variable is written to the standard output stream. If no data has been received, the asynchronous operation is canceled with <methodname>cancel</methodname><indexterm><primary>mpi::request::cancel</primary></indexterm><indexterm><primary>cancel, mpi::request</primary></indexterm>.</para>
    <para>If you run <xref linkend="ex.mpi_07" xrefstyle="enp"/> multiple times, sometimes <computeroutput>Hello, world!</computeroutput> is displayed and sometimes there is no output. The outcome depends on whether data is received before <methodname>test</methodname> is called.</para>
    <example xml:id="ex.mpi_08">
      <title>Waiting for multiple asynchronous operations with <function>wait_all</function></title>
      <programlisting><xi:include href="../src/mpi_08/main.cpp" parse="text"/></programlisting>
    </example>
    <para>You can call <methodname>test</methodname> on <classname>boost::mpi::request</classname> multiple times to detect when an asynchronous operation is complete. However, you can also call the blocking function <function>boost::mpi::wait_all</function><indexterm><primary>mpi::wait_all</primary></indexterm><indexterm><primary>wait_all, mpi</primary></indexterm> as in <xref linkend="ex.mpi_08" xrefstyle="enp"/>. <function>boost::mpi::wait_all</function> is a blocking function, but the advantage is that it can wait for multiple asynchronous operations to complete. <function>boost::mpi::wait_all</function> returns when all asynchronous operations it is waiting for have been completed.</para>
    <para>In <xref linkend="ex.mpi_08" xrefstyle="enp"/>, the process with rank 1 sends <quote>Hello, world!</quote> and the process with rank 2 <quote>Hello, moon!</quote> Since the order in which data is received doesn’t matter, the process with rank 0 calls <methodname>irecv</methodname>. Since the program will only generate output when all asynchronous operations have been completed and all data has been received, the return values of type <classname>boost::mpi::request</classname> are passed to <function>boost::mpi::wait_all</function>. Because <function>boost::mpi::wait_all</function> expects two iterators, the objects of type <classname>boost::mpi::request</classname> are stored in an array. The begin and end iterators are passed to <function>boost::mpi::wait_all</function>.</para>
    <para>Boost.MPI provides additional functions you can use to wait for the completion of asynchronous operations. <function>boost::mpi::wait_any</function><indexterm><primary>mpi::wait_any</primary></indexterm><indexterm><primary>wait_any, mpi</primary></indexterm> returns when exactly one asynchronous operation is complete, and <function>boost::mpi::wait_some</function><indexterm><primary>mpi::wait_some</primary></indexterm><indexterm><primary>wait_some, mpi</primary></indexterm> returns when at least one asynchronous operation has been completed. Both functions return a <classname>std::pair</classname> that indicates which operation or operations are complete.</para>
    <para><function>boost::mpi::test_all</function><indexterm><primary>mpi::test_all</primary></indexterm><indexterm><primary>test_all, mpi</primary></indexterm>, <function>boost::mpi::test_any</function><indexterm><primary>mpi::test_any</primary></indexterm><indexterm><primary>test_any, mpi</primary></indexterm>, and <function>boost::mpi::test_some</function><indexterm><primary>mpi::test_some</primary></indexterm><indexterm><primary>test_some, mpi</primary></indexterm> test the status of multiple asynchronous operations with a single call. These functions are non-blocking and return immediately.</para>
  </sect1>
  <sect1 xml:id="boost.mpi-collective-data-exchange">
    <title>Collective Data Exchange</title>
    <para>The functions introduced so far share a one-to-one relationship: that is, one process sends and one process receives. The link is established through a tag. This section introduces functions that are called with the same parameters in multiple processes but execute different operations. For one process the function might send data, for another process it might receive data. These functions are called <emphasis role="concept">collective operations</emphasis><indexterm><primary>collective operations, Boost.MPI</primary></indexterm>.</para>
    <example xml:id="ex.mpi_09">
      <title>Receiving data from multiple processes with <function>gather</function></title>
      <programlisting><xi:include href="../src/mpi_09/main.cpp" parse="text"/></programlisting>
    </example>
    <para><xref linkend="ex.mpi_09" xrefstyle="enp"/> calls the function <function>boost::mpi::gather</function><indexterm><primary>mpi::gather</primary></indexterm><indexterm><primary>gather, mpi</primary></indexterm> in multiple processes. Whether the function sends or receives depends on the parameters.</para>
    <para>The processes with the ranks 1 and 2 use <function>boost::mpi::gather</function> to send data. They pass, as parameters, the data being sent &#x2013; the strings <quote>Hello, world!</quote> and <quote>Hello, moon!</quote> &#x2013; and the rank of the process the data should be transmitted to. Since <function>boost::mpi::gather</function> isn’t a member function, the communicator <varname>world</varname> also has to be passed.</para>
    <para>The process with rank 0 calls <function>boost::mpi::gather</function> to receive data. Since the data has to be stored somewhere, an object of type <type>std::vector&lt;std::string&gt;</type> is passed. Please note that you have to use this type with <function>boost::mpi::gather</function>. No other containers or string types are supported.</para>
    <para>The process with rank 0 has to pass the same parameters as the processes with rank 1 and 2. That’s why the process with rank 0 also passes <varname>world</varname>, a string to send, and 0 to <function>boost::mpi::gather</function>.</para>
    <para>If you start <xref linkend="ex.mpi_09" xrefstyle="enp" /> with three processes, <computeroutput>Hello, world!</computeroutput> and <computeroutput>Hello, moon!</computeroutput> are displayed. If you look at the output carefully, you’ll notice that an empty line is written first. The first line is the empty string the process with rank 0 passes to <function>boost::mpi::gather</function>. There are three strings in <varname>v</varname> which were received from the processes with the ranks 0, 1 and 2. The indexes of the elements in the vector correspond to the ranks of the processes. If you run the example multiple times, you’ll always get an empty string as a first element in the vector, <quote>Hello, world!</quote> as the second element and <quote>Hello, moon!</quote> as the third one.</para>
    <para>Please note that you must not run <xref linkend="ex.mpi_09" xrefstyle="enp"/> with more than three processes. If you start <application class="software">mpiexec</application> with, for example, <literal>-n 4</literal>, no data is displayed. The program will hang and will have to be aborted with <keycombo action="simul"><keycap>CTRL</keycap><keycap>C</keycap></keycombo>.</para>
    <para>Collective operations must be executed for all processes. If your program calls a function such as <function>boost::mpi::gather</function>, you have to make sure that the function is called in all processes. Otherwise it’s a violation of the <acronym>MPI</acronym> standard. Because a function like <function>boost::mpi::gather</function> has to be called by all processes, the call is usually not different per process, as in <xref linkend="ex.mpi_09" xrefstyle="enp"/>. Compare the previous example with <xref linkend="ex.mpi_10" xrefstyle="enp"/>, which does the same thing.</para>
    <example xml:id="ex.mpi_10">
      <title>Collecting data from all processes with <function>gather</function></title>
      <programlisting><xi:include href="../src/mpi_10/main.cpp" parse="text"/></programlisting>
    </example>
    <para>You call functions for collective operations in all processes. Usually the functions are defined in a way that it’s clear which operation has to be executed, even if all processes pass the same parameters.</para>
    <para><xref linkend="ex.mpi_10" xrefstyle="enp"/> uses <function>boost::mpi::gather</function>, which gathers data. The data is gathered in the process whose rank is passed as the last parameter to <function>boost::mpi::gather</function>. This process gathers the data it receives from all processes. The vector to store data is used exclusively by the process that gathers data.</para>
    <para><function>boost::mpi::gather</function> gathers data from all processes. This includes the process that gathers data. In <xref linkend="ex.mpi_10" xrefstyle="enp"/>, that is the process with rank 0. This process sends an empty string to itself in <varname>s</varname>. The empty string is stored in <varname>v</varname>. As you’ll see in the following examples, collective operations always include all processes.</para>
    <para>You can run <xref linkend="ex.mpi_10" xrefstyle="enp"/> with as many processes as you like because every process calls <function>boost::mpi::gather</function>. If you run the example with three processes, the result will be similar to the previous example.</para>
    <example xml:id="ex.mpi_11">
      <title>Scattering data with <function>scatter</function> across all processes</title>
      <programlisting><xi:include href="../src/mpi_11/main.cpp" parse="text"/></programlisting>
    </example>
    <para><xref linkend="ex.mpi_11" xrefstyle="enp"/> introduces the function <function>boost::mpi::scatter</function><indexterm><primary>mpi::scatter</primary></indexterm><indexterm><primary>scatter, mpi</primary></indexterm>. It does the opposite of <function>boost::mpi::gather</function>. While <function>boost::mpi::gather</function> gathers data from multiple processes in one process, <function>boost::mpi::scatter</function> scatters data from one process across multiple processes.</para>
    <para><xref linkend="ex.mpi_11" xrefstyle="enp"/> scatters the data in <varname>v</varname> from the process with rank 0 across all processes, including itself. The process with rank 0 receives the string <quote>Hello, world!</quote> in <varname>s</varname>, the process with rank 1 receives <quote>Hello, moon!</quote> in <varname>s</varname>, and the process with rank 2 receives <quote>Hello, sun!</quote> in <varname>s</varname>.</para>
    <example xml:id="ex.mpi_12">
      <title>Sending data to all processes with <function>broadcast</function></title>
      <programlisting><xi:include href="../src/mpi_12/main.cpp" parse="text"/></programlisting>
    </example>
    <para><function>boost::mpi::broadcast</function><indexterm><primary>mpi::broadcast</primary></indexterm><indexterm><primary>broadcast, mpi</primary></indexterm> sends data from a process to all processes. The difference between this function and <function>boost::mpi::scatter</function> is that the same data is sent to all processes. In <xref linkend="ex.mpi_12" xrefstyle="enp"/>, all processes receive the string <quote>Hello, world!</quote> in <varname>s</varname> and write <computeroutput>Hello, world!</computeroutput> to the standard output stream.</para>
    <example xml:id="ex.mpi_13">
      <title>Gathering and analyzing data with <function>reduce</function></title>
      <programlisting><xi:include href="../src/mpi_13/main.cpp" parse="text"/></programlisting>
    </example>
    <para><function>boost::mpi::reduce</function><indexterm><primary>mpi::reduce</primary></indexterm><indexterm><primary>reduce, mpi</primary></indexterm> gathers data from multiple processes like <function>boost::mpi::gather</function>. However, the data isn’t stored in a vector. <function>boost::mpi::reduce</function> expects a function or function object, which it will use to analyze the data.</para>
    <para>If you run <xref linkend="ex.mpi_13" xrefstyle="enp"/> with three processes, the process with rank 0 receives the string <quote>Hello, sun!</quote> in <varname>result</varname>. The call to <function>boost::mpi::reduce</function> gathers and analyzes the strings that all of the processes pass to it. They are analyzed using the function <function>min</function>, which is passed as the fourth parameter to <function>boost::mpi::reduce</function>. <function>min</function> compares two strings and returns the shorter one.</para>
    <para>If you run <xref linkend="ex.mpi_13" xrefstyle="enp"/> with more than three processes, an empty string is displayed because all processes with a rank greater than 2 will pass an empty string to <function>boost::mpi::reduce</function>. The empty string will be displayed because it is shorter than <quote>Hello, sun!</quote></para>
    <example xml:id="ex.mpi_14">
      <title>Gathering and analyzing data with <function>all_reduce</function></title>
      <programlisting><xi:include href="../src/mpi_14/main.cpp" parse="text"/></programlisting>
    </example>
    <para><xref linkend="ex.mpi_14" xrefstyle="enp"/> uses the function <function>boost::mpi::all_reduce</function><indexterm><primary>mpi::all_reduce</primary></indexterm><indexterm><primary>all_reduce, mpi</primary></indexterm>, which gathers and analyzes data like <function>boost::mpi::reduce</function>. The difference between the two functions is that <function>boost::mpi::all_reduce</function> sends the result of the analysis to all processes while <function>boost::mpi::reduce</function> makes the result only available to the process whose rank is passed as the last parameter. Thus, no rank is passed to <function>boost::mpi::all_reduce</function>. If you run <xref linkend="ex.mpi_14" xrefstyle="enp"/> with three processes, every process writes <computeroutput>Hello, sun!</computeroutput> to the standard output stream.</para>
  </sect1>
  <sect1 xml:id="boost.mpi-communicators">
    <title>Communicators</title>
    <para>All of the examples in this chapter use only one communicator, which links all processes. However, it is possible to create more communicators to link subsets of processes. This is especially useful for collective operations that don’t need to be executed by all processes.</para>
    <example xml:id="ex.mpi_15">
      <title>Working with multiple communicators</title>
      <programlisting><xi:include href="../src/mpi_15/main.cpp" parse="text"/></programlisting>
    </example>
    <para><xref linkend="ex.mpi_15" xrefstyle="enp"/> uses the function <function>boost::mpi::broadcast</function>. This function sends the string <quote>Hello, world!</quote> from the process with rank 0 to all processes that are linked to the communicator <varname>local</varname>. The process with rank 0 has to be linked to that communicator, too.</para>
    <para>The communicator <varname>local</varname> is created by a call to <methodname>split</methodname><indexterm><primary>mpi::communicator::split</primary></indexterm><indexterm><primary>split, mpi::communicator</primary></indexterm>. <methodname>split</methodname> is a member function called on the global communicator <varname>world</varname>. <methodname>split</methodname> expects an integer to link processes together. All processes that pass the same integer to <methodname>split</methodname> are linked to the same communicator. The value of the integer passed to <methodname>split</methodname> doesn’t matter. All that’s important is that all processes that should be linked by a particular communicator pass the same value.</para>
    <para>In <xref linkend="ex.mpi_15" xrefstyle="enp"/>, the two processes with the ranks 0 and 1 pass 99 to <methodname>split</methodname>. If the program is started with more than two processes, the additional processes pass 100. This means the first two processes have one local communicator and all the other process have another local communicator. Every process is linked to the communicator that is returned by <methodname>split</methodname>. Whether there are other processes linked to the same communicator depends on whether other processes passed the same integer to <methodname>split</methodname>.</para>
    <para>Please note that ranks are always relative to a communicator. The lowest rank is always 0. In <xref linkend="ex.mpi_15" xrefstyle="enp"/>, the process with the rank 0 relative to the global communicator has also the rank 0 relative to its local communicator. The process with the rank 2 relative to the global communicator has the rank 0 relative to its local communicator.</para>
    <para>If you start <xref linkend="ex.mpi_15" xrefstyle="enp" /> with two or more processes, <computeroutput>Hello, world!</computeroutput> will be displayed twice &#x2013; once each by the processes with the ranks 0 and 1 relative to the global communicator. Because <varname>s</varname> is set to <quote>Hello, world!</quote> only in the process with the global rank 0, this string is sent through the communicator only to those processes that are linked to the same communicator. This is just the process with global rank 1, which is the only other process that passed 99 to <methodname>split</methodname>.</para>
    <example xml:id="ex.mpi_16">
      <title>Grouping processes with <classname>group</classname></title>
      <programlisting><xi:include href="../src/mpi_16/main.cpp" parse="text"/></programlisting>
    </example>
    <para>MPI supports grouping processes. This is done with the help of the class <classname>boost::mpi::group</classname><indexterm><primary>mpi::group</primary></indexterm><indexterm><primary>group, mpi</primary></indexterm>. If you call the member function <methodname>group</methodname><indexterm><primary>mpi::communicator::group</primary></indexterm><indexterm><primary>group, mpi::communicator</primary></indexterm> on a communicator, all processes linked to the communicator are returned in an object of type <classname>boost::mpi::group</classname>. You can’t use this object for communication. It can only be used to form a new group of processes from which a communicator can then be created.</para>
    <para><classname>boost::mpi::group</classname> provides member functions like <methodname>include</methodname><indexterm><primary>mpi::group::include</primary></indexterm><indexterm><primary>include, mpi::group</primary></indexterm> and <methodname>exclude</methodname><indexterm><primary>mpi::group::exclude</primary></indexterm><indexterm><primary>exclude, mpi::group</primary></indexterm>. You pass iterators to include or exclude processes. <methodname>include</methodname> and <methodname>exclude</methodname> return a new group of type <classname>boost::mpi::group</classname>.</para>
    <para><xref linkend="ex.mpi_16" xrefstyle="enp"/> passes two iterators to <methodname>exclude</methodname> that refer to an object of type <classname>boost::integer_range</classname>. This object represents a range of integers. It is created with the help of the function <function>boost::irange</function>, which expects a lower and upper bound. The upper bound is an integer that doesn’t belong to the range. In this example, this means that <varname>r</varname> only includes the integer 0.</para>
    <para>The call to <methodname>exclude</methodname> results in <varname>subgroup</varname> being created, which contains all processes except the one with the rank 0. This group is then used to create a new communicator <varname>others</varname>. This is done by passing the global communicator <varname>world</varname> and <varname>subgroup</varname> to the constructor of <classname>boost::mpi::communicator</classname>.</para>
    <para>Please note that <varname>others</varname> is a communicator that is empty in the process with rank 0. The process with rank 0 isn’t linked to this communicator, but the variable <varname>others</varname> still exists in this process. You must be careful not to use <varname>others</varname> in this process. <xref linkend="ex.mpi_16" xrefstyle="enp" /> prevents this by calling <methodname>rank</methodname><indexterm><primary>mpi::group::rank</primary></indexterm><indexterm><primary>rank, mpi::group</primary></indexterm> on <varname>subgroup</varname>. The member function returns an empty object of type <classname>boost::optional</classname> in processes that don’t belong to the group. Other processes receive their rank relative to the group.</para>
    <para>If <methodname>rank</methodname> returns a rank and no empty object of type <classname>boost::optional</classname>, <function>boost::mpi::broadcast</function> is called. The process with rank 0 sends the string <quote>Hello, world!</quote> to all processes linked to the communicator <varname>others</varname>. Please note that the rank is relative to that communicator. The process with the rank 0 relative to <varname>others</varname> has the rank 1 relative to the global communicator <varname>world</varname>.</para>
    <para>If you run <xref linkend="ex.mpi_16" xrefstyle="enp"/> with more than two processes, all processes with a global rank greater than 0 will display <computeroutput>Hello, world!</computeroutput>.</para>
  </sect1>
</chapter>
